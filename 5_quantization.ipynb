{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ML-HW-SYS/a2/blob/main/5_quantization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLNHtJrAqKWo"
      },
      "source": [
        "# **5. Quantization**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5kS114ooq-0"
      },
      "source": [
        "## 5.0 Setup Capabilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zjOhFCIkxVfL"
      },
      "outputs": [],
      "source": [
        "# Mount google drive \n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hoEUDK6KxVfM"
      },
      "outputs": [],
      "source": [
        "# Make sure your token is stored in a txt file at the location below.\n",
        "# This way there is no risk that you will push it to your repo\n",
        "# Never share your token with anyone, it is basically your github password!\n",
        "with open('/content/gdrive/MyDrive/ece5545/token.txt') as f:\n",
        "    token = f.readline().strip()\n",
        "# Use another file to store your github username    \n",
        "with open('/content/gdrive/MyDrive/ece5545/git_username.txt') as f:\n",
        "    handle = f.readline().strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lXP6DjaqxVfM"
      },
      "outputs": [],
      "source": [
        "# Clone your github repo\n",
        "YOUR_TOKEN = token\n",
        "YOUR_HANDLE = handle\n",
        "BRANCH = \"main\"\n",
        "\n",
        "%mkdir /content/gdrive/MyDrive/ece5545\n",
        "%cd /content/gdrive/MyDrive/ece5545\n",
        "!git clone https://{YOUR_TOKEN}@github.com/ML-HW-SYS/a2-{YOUR_HANDLE}.git\n",
        "%cd /content/gdrive/MyDrive/ece5545/a2-{YOUR_HANDLE}\n",
        "!git checkout {BRANCH}\n",
        "!git pull\n",
        "%cd /content/gdrive/MyDrive/ece5545\n",
        "\n",
        "PROJECT_ROOT = f\"/content/gdrive/MyDrive/ece5545/a2-{YOUR_HANDLE}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6eDWK1EVxVfN"
      },
      "outputs": [],
      "source": [
        "# This extension reloads all imports before running each cell\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4w6cL9l3xVfO"
      },
      "source": [
        "Please verify the cell below prints out the github repository."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TQSeJ7fExVfP"
      },
      "outputs": [],
      "source": [
        "!ls '{PROJECT_ROOT}'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MM5gV8CNqd-s"
      },
      "source": [
        "### Install required packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NLE4Xmakoq-4"
      },
      "outputs": [],
      "source": [
        "# Install libraries\n",
        "!pip install tqdm\n",
        "!pip install torchaudio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55zgRQWVqu6n"
      },
      "source": [
        "### Import code dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0pk0tnhRoq-5"
      },
      "outputs": [],
      "source": [
        "# Import libraries \n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.notebook import tqdm\n",
        "import numpy as np\n",
        "import copy\n",
        "import os\n",
        "\n",
        "import sys\n",
        "\n",
        "# Adding assignment 2 to the system path\n",
        "# -- make sure this matches your git directory\n",
        "sys.path.insert(0, PROJECT_ROOT)\n",
        "\n",
        "# Import data_proc to use data processing functions\n",
        "import src.data_proc as data_proc\n",
        "\n",
        "# Import constants to use constants defined for training\n",
        "from src.constants import *\n",
        "\n",
        "# Set random seed\n",
        "# Make sure the shuffling and picking is deterministic\n",
        "# Note that different value of random_seed may change rate of variation in loss/accuracy during training\n",
        "# Using the same random seed value every time you rerun the notebook will \n",
        "# reproduce the training and testing results  \n",
        "random_seed = RANDOM_SEED\n",
        "torch.manual_seed(random_seed)\n",
        "torch.cuda.manual_seed(random_seed)\n",
        "np.random.seed(random_seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qPNyurTpoq_C"
      },
      "source": [
        "## 5.1 Define Quantization Functions\n",
        "\n",
        "There are some test cases in the `tests` folder to verify basic functionality of your implemented functions--these will be run automatically every time you check in your code. Additionally, we've left some simple tests in this notebook as well for you to try things out.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hTaCRrjeJgBR"
      },
      "source": [
        "#### TODO 0: Implement the backward pass of `ste_round` function in `src/quant.py`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VQy4ffsVPc8o"
      },
      "outputs": [],
      "source": [
        "# add a test if you like. There's already one under tests/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOvGVC1CxVfR"
      },
      "source": [
        "\n",
        "#### TODO 1: Implement the `linear_quantize` function in `src/quant.py`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EXUIpaEMoq_C"
      },
      "outputs": [],
      "source": [
        "from src.quant import linear_quantize\n",
        "\n",
        "# Mini test case for linear_quantize\n",
        "with torch.no_grad():\n",
        "    x = torch.tensor([2, -0.5, 0., 1.])\n",
        "    scale = 1\n",
        "    zero = 0\n",
        "    y = linear_quantize(x, scale, zero)\n",
        "    print(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bvZ9zUycxVfR"
      },
      "source": [
        "#### TODO 2: Implement the `SymmetricQuantFunction` forward function in `src/quant.py`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RxiSaE_dxVfR"
      },
      "outputs": [],
      "source": [
        "from src.quant import SymmetricQuantFunction\n",
        "\n",
        "quant_f = SymmetricQuantFunction.apply\n",
        "\n",
        "x = torch.tensor([2, -0.5, 0., 1.])\n",
        "x.requires_grad = True\n",
        "bw = 2\n",
        "y = quant_f(x, bw, scale, zero)\n",
        "(y ** 2).sum().backward()\n",
        "\n",
        "print(y)\n",
        "print(x.grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7M_CmQ5-xVfS"
      },
      "source": [
        "#### TODO 3: Implement the `AsymmetricQuantFunction` forward function in `src/quant.py`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XQBP2WWtxVfS"
      },
      "outputs": [],
      "source": [
        "from src.quant import AsymmetricQuantFunction\n",
        "\n",
        "quant_f = AsymmetricQuantFunction.apply\n",
        "\n",
        "x = torch.tensor([2, -0.5, 0., 1.])\n",
        "x.requires_grad = True\n",
        "bw = 2\n",
        "y = quant_f(x, bw, scale, zero)\n",
        "(y ** 2).sum().backward()\n",
        "\n",
        "print(y)\n",
        "print(x.grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EClG-iStxVfS"
      },
      "source": [
        "#### TODO 4: Finish the Implement of `get_quantization_params` function in `src/quant.py`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wtjXI5zjxVfS"
      },
      "outputs": [],
      "source": [
        "from src.quant import QConfig\n",
        "\n",
        "qconfig = QConfig(quant_bits=4, is_symmetric=True)\n",
        "print(qconfig.get_quantization_params(x.min(), x.max()))\n",
        "\n",
        "qconfig = QConfig(quant_bits=4, is_symmetric=False)\n",
        "print(qconfig.get_quantization_params(x.min(), x.max()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MLT8xONCoq_D"
      },
      "source": [
        "#### TODO 5: Implement the `quantize_weights_bias` function in `src/quant.py`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1athpSf1oq_D"
      },
      "outputs": [],
      "source": [
        "from src.quant import quantize_weights_bias, QConfig\n",
        "\n",
        "qconfig = QConfig(quant_bits=4, is_symmetric=True)\n",
        "\n",
        "w1 = nn.Parameter(torch.tensor([2, -0.5, 0., 1.]))\n",
        "qw1 = quantize_weights_bias(w1, qconfig)\n",
        "print(qw1.data)\n",
        "\n",
        "w2 = nn.Parameter(torch.tensor([2.5, -1, 0., 1.5]))\n",
        "qw2 = quantize_weights_bias(w2, qconfig)\n",
        "print(qw2.data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnpBdnuBoq_E"
      },
      "source": [
        "## 5.2 Quantization Function for Linear and Convolution Layer\n",
        "\n",
        "#### TODO 6: Finish the implementation of `conv2d_linear_quantized` function in `src/quant.py`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XNN8WiYPoq_E"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "from src.quant import QuantWrapper\n",
        "\n",
        "layer = nn.Linear(2, 2)\n",
        "layer.weight.data = torch.tensor([[0.1, 0.1], [-0.1, 0.1]]).view(2, 2).float()\n",
        "layer.bias.data = torch.tensor([1, 2]).view(*layer.bias.shape).float()\n",
        "x = torch.tensor([[0., 1]])\n",
        "print(layer(x))\n",
        "\n",
        "quant_layer = QuantWrapper(\n",
        "    layer, \n",
        "    QConfig(quant_bits=4, is_symmetric=True), \n",
        "    QConfig(quant_bits=4, is_symmetric=True), \n",
        "    QConfig(quant_bits=4, is_symmetric=True))\n",
        "print(quant_layer(x))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lax5KDyxoq_F"
      },
      "source": [
        "## 5.3 Prepare model for QAT (Quantization Aware Training)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yLFhKVStmTbG"
      },
      "source": [
        "### Get Audio Processor, Devices, Data Loader, and Model\n",
        "\n",
        "NOTE: This is identical to section 2.2 ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SBcLXHHim6ry"
      },
      "outputs": [],
      "source": [
        "# Create audio_processor\n",
        "# DATASET_DIR is defined in constants.py\n",
        "# HINT: In case loading data takes too long, move the dataset from gdrive to /content/ and change the path accordingly.\n",
        "audio_processor = data_proc.AudioProcessor(data_dir=DATASET_DIR)\n",
        "print(\"Audio_processor created\")\n",
        "\n",
        "# Define device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f'Using {device} to run the training scrpit.')\n",
        "\n",
        "# Define data loaders\n",
        "from src.loaders import make_data_loaders\n",
        "data_loaders = make_data_loaders(audio_processor, device)\n",
        "train_loader = data_loaders['training']\n",
        "test_loader = data_loaders['testing']\n",
        "valid_loader = data_loaders['validation']\n",
        "\n",
        "# Create a full precision (float32) TinyConv model\n",
        "from src.networks import TinyConv\n",
        "model_fp32 = TinyConv(model_settings=audio_processor.model_settings, \\\n",
        "    n_input=1, n_output=audio_processor.num_labels)\n",
        "\n",
        "model_fp32"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7wcn11MxVfT"
      },
      "source": [
        "### Load Pretrained Model for Quantization Aware Finetuning\n",
        "\n",
        "In this notebook, we will load the previously trained 32-bits float model to finetune it in a quantizaiton-aware way. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e_QYYl4oxVfT"
      },
      "outputs": [],
      "source": [
        "!ls {TORCH_DIR}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ys6gVPF6xVfT"
      },
      "source": [
        "### **TODO: Replace the torch_path model with the model you created in the last section.** \n",
        "\n",
        "You can find the name of your file in `TORCH_DIR` under the folder icon to the left. (Or from running the tab above)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "98TcWxPWxVfU"
      },
      "outputs": [],
      "source": [
        "# TODO: Replace me!\n",
        "torch_path = os.path.join(TORCH_DIR, \"tinyconv_float32_init_seed0_90.28%_0.pt\")\n",
        "\n",
        "# Load model\n",
        "model_fp32.load_state_dict(torch.load(torch_path))\n",
        "model_fp32"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6RTZp_yoq_F"
      },
      "source": [
        "### Define settings for weight and activation quantization "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KrTGvZMtoq_F"
      },
      "outputs": [],
      "source": [
        "# We choose 4 bit quantization as an example because accuracy improvements will \n",
        "# be more obvious with 4-bit or lower bit quantization\n",
        "QUANT_BITS = 4\n",
        "# Settings for activations quantization: n-bit asymmetric quantization\n",
        "a_qconfig = QConfig(quant_bits=QUANT_BITS, is_symmetric=False)\n",
        "# Settings for weights quantization: n-bit symmetric quantization\n",
        "w_qconfig = QConfig(quant_bits=QUANT_BITS, is_symmetric=True)\n",
        "# Settings for bias quantization: n-bit symmetric quantization\n",
        "b_qconfig = QConfig(quant_bits=QUANT_BITS, is_symmetric=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5FD7l9cNoq_F"
      },
      "source": [
        "### Prepare quantization aware training model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "capV1g7loq_F"
      },
      "outputs": [],
      "source": [
        "from src.quant import quantize_model\n",
        "qat_model_nbit = quantize_model(\n",
        "    model_fp32, a_qconfig=a_qconfig, w_qconfig=w_qconfig, b_qconfig=b_qconfig)\n",
        "\n",
        "# Print to see the model prepared for QAT\n",
        "print(qat_model_nbit)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SkHtQEQXoq_G"
      },
      "source": [
        "##  5.4 Finetuning\n",
        "\n",
        "In this training, we will finetune the 32-bits float pretrained model. The goal is to finetune the weights of the 32-bits float model such that the resulted model will have better accuracy after quantization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QvZ9g2QlxVfU"
      },
      "source": [
        "### Quantization Aware Finetuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fCC9s6dUoq_H",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "import time\n",
        "from src.train_val_test_utils import train, test\n",
        "from src.train_val_test_utils import create_optimizer\n",
        "\n",
        "\n",
        "def run_training(model, data_loaders, n_epoch, log_interval, optimizer, scheduler=None, \n",
        "                 save_interval=1, resume=True, checkpoint_path=None, verbose=False):\n",
        "    test_loader = data_loaders['testing']\n",
        "    with tqdm(total=n_epoch) as pbar:\n",
        "        completed_epoch = 1\n",
        "        if resume:\n",
        "            try:\n",
        "                #continue training with previous model if one exists\n",
        "                if checkpoint_path is None:\n",
        "                    raise ValueError\n",
        "                checkpoint = torch.load(checkpoint_path)\n",
        "                model.load_state_dict(checkpoint['model_state_dict'])\n",
        "                optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "                if scheduler is not None:\n",
        "                    scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "                completed_epoch = checkpoint[\"epoch\"] + 1\n",
        "                model.eval()\n",
        "                pbar.update(completed_epoch)\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "        for epoch in range(completed_epoch, n_epoch + 1):\n",
        "            train_iters = len(data_loaders['training'])\n",
        "            train(model, data_loaders, optimizer, epoch, device, verbose)\n",
        "            test(test_loader, model, device, \n",
        "                 epoch=None, loader_type='Test')\n",
        "            \n",
        "            if scheduler is not None:\n",
        "                scheduler.step()\n",
        "            #checkpoint the model every run\n",
        "            if epoch % save_interval == 0 and checkpoint_path is not None:\n",
        "                torch.save({\n",
        "                    'epoch': epoch,\n",
        "                    'model_state_dict': model.state_dict(),\n",
        "                    'optimizer_state_dict': optimizer.state_dict(),\n",
        "                    'scheduler_state_dict': scheduler.state_dict() if scheduler is not None else None\n",
        "                }, checkpoint_path)\n",
        "                \n",
        "            # Update epoch pbar\n",
        "            pbar.update(1)\n",
        "\n",
        "\n",
        "verbose = False\n",
        "log_interval = 100\n",
        "num_batches = len(train_loader)\n",
        "n_epoch = 30\n",
        "print(f'#batches: {num_batches} \\n#epochs: {n_epoch} \\n#total training steps: {num_batches * n_epoch}')\n",
        "\n",
        "# Create optimizer\n",
        "optimizer_quant = create_optimizer(model=qat_model_nbit, learning_rate=0.001)\n",
        "print(optimizer_quant.state_dict())\n",
        "\n",
        "checkpoint_path = os.path.join(TORCH_DIR, \"quant_checkpoint.pt\")\n",
        "qat_model_nbit.to(device)\n",
        "run_training(\n",
        "    model=qat_model_nbit, data_loaders=data_loaders, \n",
        "    n_epoch=n_epoch, log_interval=log_interval, \n",
        "    optimizer=optimizer_quant, scheduler=None, \n",
        "    resume=False,\n",
        "    checkpoint_path=checkpoint_path,\n",
        "    verbose=verbose\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rj8xFMjqxVfV"
      },
      "source": [
        "### Finetune the Float Model\n",
        "\n",
        "For fair comparison, we conduct the same funetuning for the float model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hx9VthdKxVfV",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "# Create optimizer\n",
        "optimizer_fp32 = create_optimizer(model=model_fp32, learning_rate=0.0001)\n",
        "\n",
        "checkpoint_path = os.path.join(TORCH_DIR, \"fp32_finetune_checkpoint.pt\")\n",
        "model_fp32.to(device)\n",
        "run_training(\n",
        "    model=model_fp32, data_loaders=data_loaders, \n",
        "    n_epoch=n_epoch, log_interval=log_interval, \n",
        "    optimizer=optimizer_fp32, scheduler=None, \n",
        "    resume=False,\n",
        "    checkpoint_path=checkpoint_path,\n",
        "    verbose=verbose\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jOIj0QUOoq_H"
      },
      "source": [
        "## 5.5 Testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95C3tSGNoq_H"
      },
      "source": [
        "We will compute the accuracy of the finetuned model in train/val/test set in this section.\n",
        "Note that this is not the final accuracy we want the model to perform well on. \n",
        "We would like our quantized-aware-finetuned model to perform well when quantized into integer. \n",
        "But the training/validation/testing accuracy of these model in quantization simulation model is still worth looking at for sanity checking purpose."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0eFxrBqCoq_H",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "from src.train_val_test_utils import plot_acc\n",
        "\n",
        "test_time_data_loaders = make_data_loaders(\n",
        "    audio_processor, device, \n",
        "    test_batch_size=1, valid_batch_size=1,\n",
        "    num_workers=0\n",
        ")\n",
        "\n",
        "plot_acc(\n",
        "    test_time_data_loaders['training'], qat_model_nbit, audio_processor, device,\n",
        "    \"Training\", 'n-bit Quantized TinyConv', \"float\")\n",
        "plot_acc(\n",
        "    test_time_data_loaders['validation'], qat_model_nbit, audio_processor, device,\n",
        "    \"Validation\", 'n-bit Quantized TinyConv', \"float\")\n",
        "plot_acc(\n",
        "    test_time_data_loaders['testing'], qat_model_nbit, audio_processor, device,\n",
        "    'Testing', 'n-bit Quantized TinyConv', \"float\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GnJHV-H_xVfV"
      },
      "outputs": [],
      "source": [
        "plot_acc(\n",
        "    test_time_data_loaders['training'], model_fp32, audio_processor, device,\n",
        "    \"Training\", 'FP32 FT TinyConv', \"float\")\n",
        "plot_acc(\n",
        "    test_time_data_loaders['validation'], model_fp32, audio_processor, device,\n",
        "    \"Validation\", 'FP32 FT TinyConv', \"float\")\n",
        "acc = plot_acc(\n",
        "    test_time_data_loaders['testing'], model_fp32, audio_processor, device,\n",
        "    'Testing', 'FP32 FT TinyConv', \"float\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5AehU-BsJgBV"
      },
      "source": [
        "## 5.6 Saving the Trained Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "agtFnTuTJgBV"
      },
      "outputs": [],
      "source": [
        "from src.train_val_test_utils import choose_name\n",
        "from src.quant import dequantize_model\n",
        "\n",
        "# Save the qat model\n",
        "qat_model_nbit_float = dequantize_model(qat_model_nbit)\n",
        "file_name = choose_name(\"quant\")\n",
        "# You can also define your own path\n",
        "qat_torch_path = os.path.join(TORCH_DIR, f'(QAT{QUANT_BITS}bit){file_name}.pt')\n",
        "# Save the trained n-bit qat pytorch model to PATH\n",
        "torch.save(qat_model_nbit.state_dict(), qat_torch_path)\n",
        "qat_torch_path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mij775vWoq_I"
      },
      "source": [
        "## 5.7 Understanding and Evaluate the Effectiveness of Quantization-Aware Training (QAT)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oUCINlWNtQB-"
      },
      "source": [
        "### Model conversion: Quantized/Float Fine-tuning Model Converted to Integer Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "odKeXjOSoq_I"
      },
      "outputs": [],
      "source": [
        "from src.quant import dequantize_model\n",
        "from src.quant_conversion import convert_to_int"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ssFRMhToq_I"
      },
      "outputs": [],
      "source": [
        "# Convert to quantized model\n",
        "\n",
        "# Quantized integer model of qat_model_nbit (quantized aware finetuning model)\n",
        "int_model_nbit = convert_to_int(\n",
        "    qat_model_nbit, QUANT_BITS, dtype=torch.int32) \n",
        "    \n",
        "# Post quantized model of model_fp32 (full-precision finetuned model)\n",
        "post_quant_model = convert_to_int(\n",
        "    model_fp32, QUANT_BITS, dtype=torch.int32) \n",
        "  \n",
        "# Floating point models of the qat_model_nbit, without QuantWrappers\n",
        "float_model_nbit = dequantize_model(qat_model_nbit)\n",
        "\n",
        "print(int_model_nbit)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jh2NGspeoq_I"
      },
      "outputs": [],
      "source": [
        "from src.quant_conversion import print_features\n",
        "\n",
        "# Select a sample data to see the features of it\n",
        "sample_data, _ = audio_processor.get_data_from_file(\n",
        "    audio_processor.data_index['testing'][0], BACKGROUND_FREQUENCY,\n",
        "    BACKGROUND_VOLUME_RANGE, TIME_SHIFT_SAMPLE, 'testing')\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"Features from Quantized QAT Model\")\n",
        "print(\"-\" * 80)\n",
        "print_features(sample_data, int_model_nbit, 'Quantized QAT Model')\n",
        "print()\n",
        "print(\"=\" * 80)\n",
        "print(\"Features from Model fp32\")\n",
        "print(\"-\" * 80)\n",
        "print_features(sample_data, model_fp32, \"Model fp32\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TMm7grfnoq_I"
      },
      "source": [
        "### Compare the Performance Between Integer Models from Float/Quantized-Aware Finetuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kHJaISbYoq_J"
      },
      "outputs": [],
      "source": [
        "from src.quant_conversion import compare_model, compare_model_mse\n",
        "\n",
        "# Compare differences in predictions\n",
        "# QAT trained floating point model vs. integer model converted from the QAT model \n",
        "# Percentage of same predictions shows how \"quantization aware\" the float point model is \n",
        "_ = compare_model(test_loader, float_model_nbit, int_model_nbit)\n",
        "_ = compare_model_mse(test_loader, float_model_nbit, int_model_nbit)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RNikQddjoq_J"
      },
      "outputs": [],
      "source": [
        "from src.quant_conversion import compare_model, compare_model_mse\n",
        "\n",
        "# Float32 model vs. integer model converted from the float32 model using post training quantization\n",
        "_ = compare_model(test_loader, model_fp32, post_quant_model)\n",
        "_ = compare_model_mse(test_loader, model_fp32, post_quant_model)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "include_colab_link": true,
      "name": "5_quantization.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "eed6bdaff5cc743acd241b8f3fe4c5dc3266475018a66ac615ca19ad2f28387d"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
