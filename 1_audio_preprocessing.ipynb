{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ML-HW-SYS/a2/blob/main/1_audio_preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pkMGw2fyVugK"
      },
      "source": [
        "# **1. Preprocessing: Audio Recording and Visualization**\n",
        "\n",
        "*You do not need to write any code for this section; simply run and explore the impact of the code below.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18_Nc-hekV08"
      },
      "source": [
        "## 1.0 Setup GDrive and Git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F90zXrTAkV08"
      },
      "outputs": [],
      "source": [
        "# Mount google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Make sure your token is stored in a txt file at the location below.\n",
        "# This way there is no risk that you will push it to your repo\n",
        "# Never share your token with anyone, it is basically your github password!\n",
        "with open('/content/gdrive/MyDrive/ece5545/token.txt') as f:\n",
        "    token = f.readline().strip()\n",
        "# Use another file to store your github username    \n",
        "with open('/content/gdrive/MyDrive/ece5545/git_username.txt') as f:\n",
        "    handle = f.readline().strip()"
      ],
      "metadata": {
        "id": "o6JpZLvpmK9p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clone your github repo\n",
        "YOUR_TOKEN = token\n",
        "YOUR_HANDLE = handle\n",
        "BRANCH = \"main\"\n",
        "\n",
        "%mkdir /content/gdrive/MyDrive/ece5545\n",
        "%cd /content/gdrive/MyDrive/ece5545\n",
        "!git clone https://{YOUR_TOKEN}@github.com/ML-HW-SYS/a2-{YOUR_HANDLE}.git\n",
        "%cd /content/gdrive/MyDrive/ece5545/a2-{YOUR_HANDLE}\n",
        "!git checkout {BRANCH}\n",
        "!git pull\n",
        "%cd /content/gdrive/MyDrive/ece5545\n",
        "\n",
        "PROJECT_ROOT = f\"/content/gdrive/MyDrive/ece5545/a2-{YOUR_HANDLE}\""
      ],
      "metadata": {
        "id": "KLAp01aclYxm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This extension reloads all imports before running each cell\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "metadata": {
        "id": "N1ZF6Legof-5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z9GCyPWNuOm7"
      },
      "source": [
        "## 1.1 Setup Recording Capabilities\n",
        "\\*Start clearing your throat: *ahem-ahem*\\*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "upgCc3gXybsA"
      },
      "source": [
        "### Install required packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uUDYyMZRfkX4"
      },
      "outputs": [],
      "source": [
        "!pip install ffmpeg-python &> 0\n",
        "!pip install torchaudio\n",
        "!pip install pydub \n",
        "print(\"Packages Installed\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bGzzi-siATEZ"
      },
      "source": [
        "\n",
        "### Import code dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WV8869MMAZKa"
      },
      "outputs": [],
      "source": [
        "from IPython.display import HTML, Audio\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode\n",
        "import numpy as np\n",
        "import io\n",
        "import ffmpeg\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import cm\n",
        "import pickle\n",
        "import librosa\n",
        "import torch\n",
        "import torchaudio\n",
        "import matplotlib.pyplot as plt\n",
        "import IPython.display as ipd\n",
        "import torchaudio.transforms as T\n",
        "import torch.fft\n",
        "import pydub\n",
        "print(\"Packages Imported\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJPb0BZuAcit"
      },
      "source": [
        "### Define the audio recording function\n",
        "This function allows you to manually record audio samples to use as your dataset. The decoding part refers to decoding the bytes-like object and returning the decoded bytes. The bytes are then converted into .wav files which are used for the audio samples.\n",
        "\n",
        "Adapted from: https://pytorch.org/tutorials/intermediate/speech_command_recognition_with_torchaudio_tutorial.html#formatting-the-data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z3TCWgioj1WD"
      },
      "outputs": [],
      "source": [
        "def record(seconds=1):\n",
        "    from google.colab import output as colab_output\n",
        "    from base64 import b64decode\n",
        "    from io import BytesIO\n",
        "    from pydub import AudioSegment\n",
        "\n",
        "    RECORD = (\n",
        "        b\"const sleep  = time => new Promise(resolve => setTimeout(resolve, time))\\n\"\n",
        "        b\"const b2text = blob => new Promise(resolve => {\\n\"\n",
        "        b\"  const reader = new FileReader()\\n\"\n",
        "        b\"  reader.onloadend = e => resolve(e.srcElement.result)\\n\"\n",
        "        b\"  reader.readAsDataURL(blob)\\n\"\n",
        "        b\"})\\n\"\n",
        "        b\"var record = time => new Promise(async resolve => {\\n\"\n",
        "        b\"  stream = await navigator.mediaDevices.getUserMedia({ audio: true })\\n\"\n",
        "        b\"  recorder = new MediaRecorder(stream)\\n\"\n",
        "        b\"  chunks = []\\n\"\n",
        "        b\"  recorder.ondataavailable = e => chunks.push(e.data)\\n\"\n",
        "        b\"  recorder.start()\\n\"\n",
        "        b\"  await sleep(time)\\n\"\n",
        "        b\"  recorder.onstop = async ()=>{\\n\"\n",
        "        b\"    blob = new Blob(chunks)\\n\"\n",
        "        b\"    text = await b2text(blob)\\n\"\n",
        "        b\"    resolve(text)\\n\"\n",
        "        b\"  }\\n\"\n",
        "        b\"  recorder.stop()\\n\"\n",
        "        b\"})\"\n",
        "    )\n",
        "    \n",
        "    RECORD = RECORD.decode(\"ascii\")\n",
        "\n",
        "    print(f\"Recording started for {seconds} seconds.\")\n",
        "    display(ipd.Javascript(RECORD))\n",
        "    s = colab_output.eval_js(\"record(%d)\" % (seconds * 1000))\n",
        "    print(\"Recording ended.\")\n",
        "    b = b64decode(s.split(\",\")[1])\n",
        "\n",
        "    fileformat = \"wav\"\n",
        "    filename = f\"_audio.{fileformat}\"\n",
        "    AudioSegment.from_file(BytesIO(b)).export(filename, format=fileformat)\n",
        "    return torchaudio.load(filename)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AY-xYPiLAj5j"
      },
      "source": [
        "## 1.2 Load in the Audio Samples\n",
        "### Record your own audio samples!\n",
        "You will have 1 second to record \"yes\"/\"no\" at \"loud\"/\"quiet\" volumes in the respective cells. The recording starts immediately, but you can re-record as many times as you would like.\n",
        "\n",
        "**If you do not want to record audio, then see below for a way to load in the default audio used in the lecture slides.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D-SSG2lv5Mg9"
      },
      "outputs": [],
      "source": [
        "audio_yes_loud, sr_yes_loud = record()\n",
        "print(\"DONE\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GyB7qaDM6iQj"
      },
      "outputs": [],
      "source": [
        "audio_yes_quiet, sr_yes_quiet = record()\n",
        "print(\"DONE\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EhtrUYWT6ti6"
      },
      "outputs": [],
      "source": [
        "audio_no_loud, sr_no_loud = record()\n",
        "print(\"DONE\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wO4wQp9D98Ds"
      },
      "outputs": [],
      "source": [
        "audio_no_quiet, sr_no_quiet = record()\n",
        "print(\"DONE\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5FKjT0VHZLvD"
      },
      "source": [
        "You can hear your recorded audio files by running the below code. Make sure the recording doesn't cut off your \"yes\"/\"no\" statements."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "toHWCy1JZgzH"
      },
      "outputs": [],
      "source": [
        "Audio(audio_yes_loud, rate=sr_yes_loud)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YUPW18JEZr33"
      },
      "outputs": [],
      "source": [
        "Audio(audio_yes_quiet, rate=sr_yes_quiet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BhsfxQk5ZsB_"
      },
      "outputs": [],
      "source": [
        "Audio(audio_no_loud, rate=sr_no_loud)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NPcNYzcaZsJy"
      },
      "outputs": [],
      "source": [
        "Audio(audio_no_quiet, rate=sr_no_quiet)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VBE2WWudAo4D"
      },
      "source": [
        "## 1.3 Visualize the Audio Samples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWUG9zu8k-lK"
      },
      "source": [
        "### Time domain\n",
        "This part of the notebook represents the .wav audio samples as signals plotted on a graph. The x-axis of the graphs represent time and is derived using frame rate. The graphs depict the magnitudes of sound of the audio samples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "il58WctF5fPV"
      },
      "outputs": [],
      "source": [
        "# Plot the figures\n",
        "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(nrows=2, ncols=2)\n",
        "tensor_cat = torch.cat((torch.cat((torch.cat((audio_yes_loud,audio_yes_quiet), 1),audio_no_loud),1),audio_no_quiet), 1)\n",
        "max_val = tensor_cat.max().item()\n",
        "\n",
        "ax1.plot(audio_yes_loud.t().numpy())\n",
        "ax1.set_title(\"Yes Loud\", {'fontsize':20, 'fontweight':'bold'})\n",
        "ax1.set_ylim(-max_val, max_val)\n",
        "\n",
        "ax2.plot(audio_yes_quiet.t().numpy())\n",
        "ax2.set_title(\"Yes Quiet\", {'fontsize':20, 'fontweight':'bold'})\n",
        "ax2.set_ylim(-max_val, max_val)\n",
        "\n",
        "ax3.plot(audio_no_loud.t().numpy())\n",
        "ax3.set_title(\"No Loud\", {'fontsize':20, 'fontweight':'bold'})\n",
        "ax3.set_ylim(-max_val, max_val)\n",
        "\n",
        "ax4.plot(audio_no_quiet.t().numpy())\n",
        "ax4.set_title(\"No Quiet\", {'fontsize':20, 'fontweight':'bold'})\n",
        "ax4.set_ylim(-max_val, max_val)\n",
        "\n",
        "fig.set_size_inches(18,12)\n",
        "fig.text(0.5, 0.06, 'Time', {'fontsize':20, 'fontweight':'bold'}, ha='center');\n",
        "fig.text(0.08, 0.5, 'Intensity', {'fontsize':20, 'fontweight':'bold'}, va='center', rotation='vertical');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61oIZujzLU4d"
      },
      "source": [
        "### Frequency domain\n",
        "Here, the audio samples are displayed in the frequency domain through Fourier transformations. The x axis, instead of representing time, now represents frequency, and the y axis represents the amount of decibels in the sample at each frequency.\n",
        "\n",
        "Adapted from: https://makersportal.com/blog/2018/9/13/audio-processing-in-python-part-i-sampling-and-the-fast-fourier-transform"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gf18UcyhM571"
      },
      "outputs": [],
      "source": [
        "# Compute the FFT and take the single-sided spectrum only and remove imaginary part\n",
        "ft_audio_yes_loud = torch.abs(2*torch.fft.fft(audio_yes_loud.t()))\n",
        "ft_audio_yes_quiet = torch.abs(2*torch.fft.fft(audio_yes_quiet.t()))\n",
        "ft_audio_no_loud = torch.abs(2*torch.fft.fft(audio_no_loud.t()))\n",
        "ft_audio_no_quiet = torch.abs(2*torch.fft.fft(audio_no_quiet.t()))\n",
        "\n",
        "# Plot the figures\n",
        "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(nrows=2, ncols=2)\n",
        "\n",
        "ax1.plot(ft_audio_yes_loud)\n",
        "ax1.set_xscale('log')\n",
        "ax1.set_yscale('log')\n",
        "ax1.set_title(\"Yes Loud\", {'fontsize':20, 'fontweight':'bold'})\n",
        "\n",
        "ax2.plot(ft_audio_yes_quiet)\n",
        "ax2.set_xscale('log')\n",
        "ax2.set_yscale('log')\n",
        "ax2.set_title(\"Yes Quiet\", {'fontsize':20, 'fontweight':'bold'})\n",
        "\n",
        "ax3.plot(ft_audio_no_loud)\n",
        "ax3.set_xscale('log')\n",
        "ax3.set_yscale('log')\n",
        "ax3.set_title(\"No Loud\", {'fontsize':20, 'fontweight':'bold'})\n",
        "\n",
        "ax4.plot(ft_audio_no_quiet)\n",
        "ax4.set_xscale('log')\n",
        "ax4.set_yscale('log')\n",
        "ax4.set_title(\"No Quiet\", {'fontsize':20, 'fontweight':'bold'})\n",
        "\n",
        "fig.set_size_inches(18,12)\n",
        "fig.text(0.5, 0.06, 'Hz', {'fontsize':20, 'fontweight':'bold'}, ha='center');\n",
        "fig.text(0.08, 0.5, 'Decibels', {'fontsize':20, 'fontweight':'bold'}, va='center', rotation='vertical');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ou4Jf5U1Avuo"
      },
      "source": [
        "### Spectrograms\n",
        "A spectogram is a figure which visually represents the spectrum of frequencies of a recorded audio over time. This part of the notebook produces those \"pictures\" of your audio samples. Brighter color indicates more concentration around specific frequencies. They are important in machine learning since they capture the shape and structure of audio in a format algorithms can manipulate.\n",
        "\n",
        "For more information on spectrograms use in ML, checkout: [Learning from Audio: Spectrograms](https://towardsdatascience.com/learning-from-audio-spectrograms-37df29dba98c)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZIOmPv975gYW"
      },
      "outputs": [],
      "source": [
        "# Convert to spectrogram and display\n",
        "def get_spectrogram(\n",
        "    waveform,\n",
        "    n_fft = 2048,\n",
        "    win_len = None,\n",
        "    hop_len = 16000 * 0.008,\n",
        "    power = 2.0,\n",
        "    ):\n",
        "  \n",
        "    spectrogram = T.Spectrogram(\n",
        "        n_fft=n_fft,\n",
        "        win_length=win_len,\n",
        "        hop_length=hop_len,\n",
        "        center=True,\n",
        "        pad_mode=\"reflect\",\n",
        "        power=power,\n",
        "    )\n",
        "    return spectrogram(waveform)\n",
        "\n",
        "spectrogram_yes_loud = get_spectrogram(audio_yes_loud[0], hop_len = int(sr_yes_loud * 0.008))\n",
        "spectrogram_yes_quiet = get_spectrogram(audio_yes_quiet[0], hop_len = int(sr_yes_quiet * 0.008))\n",
        "spectrogram_no_loud = get_spectrogram(audio_no_loud[0], hop_len = int(sr_no_loud * 0.008))\n",
        "spectrogram_no_quiet = get_spectrogram(audio_no_quiet[0], hop_len = int(sr_no_quiet * 0.008))\n",
        "\n",
        "# Plot the figures\n",
        "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(nrows=2, ncols=2)\n",
        "\n",
        "ax1.imshow(torch.log(spectrogram_yes_loud).numpy(), aspect='auto')\n",
        "ax1.set_title(\"Yes Loud\", {'fontsize':20, 'fontweight':'bold'})\n",
        "\n",
        "ax2.imshow(torch.log(spectrogram_yes_quiet).numpy(), aspect='auto')\n",
        "ax2.set_title(\"Yes Quiet\", {'fontsize':20, 'fontweight':'bold'})\n",
        "\n",
        "ax3.imshow(torch.log(spectrogram_no_loud).numpy(), aspect='auto')\n",
        "ax3.set_title(\"No Loud\", {'fontsize':20, 'fontweight':'bold'})\n",
        "\n",
        "ax4.imshow(torch.log(spectrogram_no_quiet).numpy(), aspect='auto')\n",
        "ax4.set_title(\"No Quiet\", {'fontsize':20, 'fontweight':'bold'})\n",
        "\n",
        "fig.set_size_inches(18,12)\n",
        "fig.text(0.5, 0.06, 'Time', {'fontsize':20, 'fontweight':'bold'}, ha='center');\n",
        "fig.text(0.08, 0.5, 'Hz', {'fontsize':20, 'fontweight':'bold'}, va='center', rotation='vertical');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g2rNE8LEcTp4"
      },
      "source": [
        "Can you see how spectrograms can help machine learning models better differentiate between audio samples?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thaEF4eV0Wol"
      },
      "source": [
        "### Mel spectrogram\n",
        "Mel spectrograms are different from spectrograms in that they display frequencies on the mel scale, which is nonlinear. The purpose of this logarithmic transformation is to account for the fact that humans do not perceive frequencies on a linear scale. Therefore, using the mel scale can help us-- and ML algorithms-- better associate the features to human hearing.\n",
        "\n",
        "Adapted from: https://towardsdatascience.com/getting-to-know-the-mel-spectrogram-31bca3e2d9d0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_m150pVm64xj"
      },
      "outputs": [],
      "source": [
        "# Torchaudio mel spectrogram\n",
        "def get_melspectrogram(\n",
        "    waveform, \n",
        "    sample_rate,\n",
        "    n_fft = 2048,\n",
        "    win_length = None,\n",
        "    hop_length = 512,\n",
        "    n_mels = 128,\n",
        "    ):\n",
        "\n",
        "    mel_spectrogram = T.MelSpectrogram(\n",
        "        sample_rate=sample_rate,\n",
        "        n_fft=n_fft,\n",
        "        win_length=win_length,\n",
        "        hop_length=hop_length,\n",
        "        center=True,\n",
        "        pad_mode=\"reflect\",\n",
        "        power=2.0,\n",
        "        norm='slaney',\n",
        "        onesided=True,\n",
        "        n_mels=n_mels,\n",
        "    )\n",
        "    return mel_spectrogram(waveform)\n",
        "\n",
        "mel_yes_loud = get_melspectrogram(waveform=audio_yes_loud[0], sample_rate=sr_yes_loud)\n",
        "mel_yes_quiet = get_melspectrogram(waveform=audio_yes_quiet[0], sample_rate=sr_yes_quiet)\n",
        "mel_no_loud = get_melspectrogram(waveform=audio_no_loud[0], sample_rate=sr_no_loud)\n",
        "mel_no_quiet = get_melspectrogram(waveform=audio_no_quiet[0], sample_rate=sr_no_quiet)\n",
        "\n",
        "# Plot the figure\n",
        "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(nrows=2, ncols=2)\n",
        "\n",
        "ax1.imshow(librosa.power_to_db(torch.swapaxes(mel_yes_loud, 0 ,1).numpy()), interpolation='nearest', cmap=cm.viridis, origin='lower', aspect='auto')\n",
        "ax1.set_title(\"Yes Loud\", {'fontsize':20, 'fontweight':'bold'})\n",
        "ax1.set_ylim(ax1.get_ylim()[::-1])\n",
        "\n",
        "ax2.imshow(librosa.power_to_db(torch.swapaxes(mel_yes_quiet, 0 ,1).numpy()), interpolation='nearest', cmap=cm.viridis, origin='lower', aspect='auto')\n",
        "ax2.set_title(\"Yes Quiet\", {'fontsize':20, 'fontweight':'bold'})\n",
        "ax2.set_ylim(ax2.get_ylim()[::-1])\n",
        "\n",
        "ax3.imshow(librosa.power_to_db(torch.swapaxes(mel_no_loud, 0 ,1).numpy()), interpolation='nearest', cmap=cm.viridis, origin='lower', aspect='auto')\n",
        "ax3.set_title(\"No Loud\", {'fontsize':20, 'fontweight':'bold'})\n",
        "ax3.set_ylim(ax3.get_ylim()[::-1])\n",
        "\n",
        "ax4.imshow(librosa.power_to_db(torch.swapaxes(mel_no_quiet, 0 ,1).numpy()), interpolation='nearest', cmap=cm.viridis, origin='lower', aspect='auto')\n",
        "ax4.set_title(\"No Quiet\", {'fontsize':20, 'fontweight':'bold'})\n",
        "ax4.set_ylim(ax4.get_ylim()[::-1])\n",
        "\n",
        "fig.set_size_inches(18,12)\n",
        "fig.text(0.5, 0.06, 'Time', {'fontsize':20, 'fontweight':'bold'}, ha='center');\n",
        "fig.text(0.08, 0.5, 'Mel Scale Frequencies', {'fontsize':20, 'fontweight':'bold'}, va='center', rotation='vertical');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bzX4FLbQD7fj"
      },
      "source": [
        "### Mel frequency cepstral coefficients (MFCC) spectrogram\n",
        "\n",
        "Similar to spectrograms, MFCC is another way to visualize sound. It bands together frequencies according to the mel scale. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i1Qkm-e6_uY_"
      },
      "outputs": [],
      "source": [
        "# Torchaudio MFCC\n",
        "def get_mfcc(\n",
        "    waveform,\n",
        "    sample_rate,\n",
        "    n_fft = 2048,\n",
        "    win_length = None,\n",
        "    hop_length = 512,\n",
        "    n_mels = 128,\n",
        "    n_mfcc = 128\n",
        "    ):\n",
        "\n",
        "    mfcc_transform = T.MFCC(\n",
        "        sample_rate=sample_rate,\n",
        "        n_mfcc=n_mfcc,\n",
        "        melkwargs={\n",
        "            'n_fft': n_fft,\n",
        "            'n_mels': n_mels,\n",
        "            'hop_length': hop_length,\n",
        "        }\n",
        "    )\n",
        "\n",
        "    mfcc = mfcc_transform(waveform)\n",
        "    return mfcc\n",
        "\n",
        "mfcc_yes_loud = get_mfcc(waveform=audio_yes_loud[0], sample_rate=sr_yes_loud)\n",
        "mfcc_yes_quiet = get_mfcc(waveform=audio_yes_quiet[0], sample_rate=sr_yes_quiet)\n",
        "mfcc_no_loud = get_mfcc(waveform=audio_no_loud[0], sample_rate=sr_no_loud)\n",
        "mfcc_no_quiet = get_mfcc(waveform=audio_no_quiet[0], sample_rate=sr_no_quiet)\n",
        "\n",
        "# Plot the figure\n",
        "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(nrows=2, ncols=2)\n",
        "\n",
        "ax1.imshow(librosa.power_to_db(torch.swapaxes(mfcc_yes_loud, 0 ,1).numpy()), interpolation='nearest', cmap=cm.viridis, origin='lower', aspect='auto')\n",
        "ax1.set_title(\"Yes Loud\", {'fontsize':20, 'fontweight':'bold'})\n",
        "ax1.set_ylim(ax1.get_ylim()[::-1])\n",
        "\n",
        "ax2.imshow(librosa.power_to_db(torch.swapaxes(mfcc_yes_quiet, 0 ,1).numpy()), interpolation='nearest', cmap=cm.viridis, origin='lower', aspect='auto')\n",
        "ax2.set_title(\"Yes Quiet\", {'fontsize':20, 'fontweight':'bold'})\n",
        "ax2.set_ylim(ax2.get_ylim()[::-1])\n",
        "\n",
        "ax3.imshow(librosa.power_to_db(torch.swapaxes(mfcc_no_loud, 0 ,1).numpy()), interpolation='nearest', cmap=cm.viridis, origin='lower', aspect='auto')\n",
        "ax3.set_title(\"No Loud\", {'fontsize':20, 'fontweight':'bold'})\n",
        "ax3.set_ylim(ax3.get_ylim()[::-1])\n",
        "\n",
        "ax4.imshow(librosa.power_to_db(torch.swapaxes(mfcc_no_quiet, 0 ,1).numpy()), interpolation='nearest', cmap=cm.viridis, origin='lower', aspect='auto')\n",
        "ax4.set_title(\"No Quiet\", {'fontsize':20, 'fontweight':'bold'})\n",
        "ax4.set_ylim(ax4.get_ylim()[::-1])\n",
        "\n",
        "fig.set_size_inches(18,12)\n",
        "fig.text(0.5, 0.06, 'Time', {'fontsize':20, 'fontweight':'bold'}, ha='center');\n",
        "fig.text(0.08, 0.5, 'Mel Scale Frequencies', {'fontsize':20, 'fontweight':'bold'}, va='center', rotation='vertical');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fps4qKDtcOJk"
      },
      "source": [
        "If you would like to learn more about Mel Spectrograms and MFCC, check out these links (as a starting point): \n",
        "* [Learning from Audio: The Mel Scale, Mel Spectrograms, and Mel Frequency Cepstral Coefficients](https://towardsdatascience.com/learning-from-audio-the-mel-scale-mel-spectrograms-and-mel-frequency-cepstral-coefficients-f5752b6324a8). \n",
        "* [Some Commonly Used Speech Feature Extraction Algorithms](https://www.intechopen.com/chapters/63970)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "1_audio_preprocessing.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "interpreter": {
      "hash": "6290d9b57a680332df40e39afe9e6c93fe3f483ec5fe8c905358840db4c03cc9"
    },
    "kernelspec": {
      "display_name": "ece5545",
      "language": "python",
      "name": "ece5545"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
