{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ML-HW-SYS/a2/blob/main/4_model_conversion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-C4wJCpajkZ"
      },
      "source": [
        "# **4. Convert PyTorch Model to TFLite**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3D6Na9yGb04O"
      },
      "source": [
        "## 4.0 Setup GDrive and Git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8krC9IgtrZG-"
      },
      "source": [
        "### Import GitHub dependencies and mount Google Drive\n",
        "You should now see your GitHub files under the folder icon to the left. Make sure to include your own token."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0y5bLnINkn3Z"
      },
      "outputs": [],
      "source": [
        "# Mount google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nY0HseSX1CHk"
      },
      "outputs": [],
      "source": [
        "# Make sure your token is stored in a txt file at the location below.\n",
        "# This way there is no risk that you will push it to your repo\n",
        "# Never share your token with anyone, it is basically your github password!\n",
        "with open('/content/gdrive/MyDrive/ece5545/token.txt') as f:\n",
        "    token = f.readline().strip()\n",
        "# Use another file to store your github username    \n",
        "with open('/content/gdrive/MyDrive/ece5545/git_username.txt') as f:\n",
        "    handle = f.readline().strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wqskK8iu1adQ"
      },
      "outputs": [],
      "source": [
        "# Clone your github repo\n",
        "YOUR_TOKEN = token\n",
        "YOUR_HANDLE = handle\n",
        "BRANCH = \"main\"\n",
        "\n",
        "%mkdir /content/gdrive/MyDrive/ece5545\n",
        "%cd /content/gdrive/MyDrive/ece5545\n",
        "!git clone https://{YOUR_TOKEN}@github.com/ML-HW-SYS/a2-{YOUR_HANDLE}.git\n",
        "%cd /content/gdrive/MyDrive/ece5545/a2-{YOUR_HANDLE}\n",
        "!git checkout {BRANCH}\n",
        "!git pull\n",
        "%cd /content/gdrive/MyDrive/ece5545\n",
        "\n",
        "PROJECT_ROOT = f\"/content/gdrive/MyDrive/ece5545/a2-{YOUR_HANDLE}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "13BIJ7Mb1gA0"
      },
      "outputs": [],
      "source": [
        "# This extension reloads all imports before running each cell\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HVFxRcvHmV6h"
      },
      "source": [
        "### Install required packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g3PSsTxqliKn",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Install ONNX\n",
        "!pip3 install onnx\n",
        "\n",
        "# Install tensorflow-addons\n",
        "!pip3 install tensorflow-addons\n",
        "!pip3 install tensorflow-probability\n",
        "\n",
        "# Install onnx-tensorflow\n",
        "!git clone https://github.com/onnx/onnx-tensorflow.git \n",
        "!cd onnx-tensorflow && pip3 install -e . \n",
        "# NOTE THERE IS A BUG. If you try and fail to import onnx_tf then just restart \n",
        "# your runtime after cloning the repo and it should work!\n",
        "\n",
        "!pip3 install torchaudio\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-6FmczBc17G"
      },
      "source": [
        "### Import code dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EgPNQ2hmliKn"
      },
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import sys,os\n",
        "\n",
        "# Add path to onnx-tensorflow\n",
        "sys.path.insert(0, os.path.join(PROJECT_ROOT, 'onnx-tensorflow'))\n",
        "sys.path.insert(0, PROJECT_ROOT)\n",
        "\n",
        "import onnx\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import os\n",
        "import tensorflow as tf\n",
        "print(f\"TensorFlow Version is: {tf.__version__}\")\n",
        "from onnx_tf.backend import prepare\n",
        "\n",
        "# Import constants to use constants defined for training\n",
        "from src.constants import *\n",
        "import src.data_proc as data_proc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFI3vPiDjP8J"
      },
      "source": [
        "## 4.1 Define the Model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "suBeSM0UlUAs"
      },
      "source": [
        "**Replace the torch_path model with the model you created in the last section.** \n",
        "\n",
        "You can find the name of your file in `TORCH_DIR` under the folder icon to the left. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P0YnTszF1CHo"
      },
      "outputs": [],
      "source": [
        "!ls {TORCH_DIR}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZfU0wDt1CHo"
      },
      "source": [
        "Please use a checkpoint from the cell above and place it in the cell below for the `torch_path`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G3tWEwuNi5Ex"
      },
      "outputs": [],
      "source": [
        "from src.networks import TinyConv\n",
        "import os.path as osp\n",
        "\n",
        "# Create audio processor\n",
        "audio_processor = data_proc.AudioProcessor(data_dir=DATASET_DIR) # DATASET_DIR is defined in constants\n",
        "\n",
        "# Device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Create a full precision (float32) TinyConv model\n",
        "model_fp32 = TinyConv(model_settings=audio_processor.model_settings, \\\n",
        "    n_input=1, n_output=audio_processor.num_labels)\n",
        "model_fp32.to(device)\n",
        "\n",
        "# TODO: Replace me!\n",
        "torch_path = osp.join(TORCH_DIR, \"tinyconv_float32_init_seed0_90.06%_0.pt\")\n",
        "\n",
        "# Load model\n",
        "model_fp32.load_state_dict(torch.load(torch_path))\n",
        "model_fp32"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WBBna3j7liKn"
      },
      "source": [
        "## 4.2 Convert from PyTorch to TFLite\n",
        "This is the last step before deploying your model to your TinyML device. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VVKrP45aliKm"
      },
      "outputs": [],
      "source": [
        "from src.train_val_test_utils import choose_name\n",
        "# Choose a path\n",
        "# You can also define your own path\n",
        "file_name = choose_name(\"temp\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MrW3BS8b6QYD"
      },
      "source": [
        "### Convert to ONNX"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W7gTPQuiliKn"
      },
      "outputs": [],
      "source": [
        "# Create a new TinyConv model\n",
        "ONNX_PATH = os.path.join(ONNX_DIR, f'{file_name}.onnx')\n",
        "TF_PATH = os.path.join(TF_DIR, f'{file_name}')\n",
        "\n",
        "# Create a random input for model export\n",
        "dummy_input = torch.autograd.Variable(torch.randn(1, 1960))\n",
        "\n",
        "# Export model as .onnx file\n",
        "torch.onnx.export(model_fp32.cpu(), dummy_input, ONNX_PATH, input_names=['in'], output_names=['out'])\n",
        "\n",
        "# Load onnx model into a tensorflow model\n",
        "onnx_model = onnx.load(ONNX_PATH)\n",
        "tf_rep = prepare(onnx_model) \n",
        "\n",
        "# Export model as .pb file\n",
        "\n",
        "tf_rep.export_graph(TF_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "owh3fmj66aTM"
      },
      "source": [
        "### Convert to TFLite"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dou6nTCHliKn"
      },
      "outputs": [],
      "source": [
        "FLOAT_MODEL_TFLITE = os.path.join(TFLITE_DIR, f'float_{file_name}.tflite')\n",
        "MODEL_TFLITE = os.path.join(TFLITE_DIR, f'quant_{file_name}.tflite')\n",
        "\n",
        "float_converter = tf.lite.TFLiteConverter.from_saved_model(TF_PATH)\n",
        "float_tflite_model = float_converter.convert()\n",
        "float_tflite_model_size = open(FLOAT_MODEL_TFLITE, \"wb\").write(float_tflite_model)\n",
        "print(\"Float model is %d bytes\" % float_tflite_model_size)\n",
        "\n",
        "converter = tf.lite.TFLiteConverter.from_saved_model(TF_PATH)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "converter.inference_input_type = tf.int8\n",
        "converter.inference_output_type = tf.int8\n",
        "def representative_dataset_gen():\n",
        "    for i in range(100):\n",
        "        data, _ = audio_processor.get_data(\n",
        "            1, i*1, BACKGROUND_FREQUENCY, BACKGROUND_VOLUME_RANGE, TIME_SHIFT_MS, 'testing')\n",
        "        flattened_data = np.array(data.flatten(), dtype=np.float32).reshape([1, 1960])\n",
        "        yield [flattened_data]\n",
        "converter.representative_dataset = representative_dataset_gen\n",
        "\n",
        "tflite_model = converter.convert()\n",
        "tflite_model_size = open(MODEL_TFLITE, \"wb\").write(tflite_model)\n",
        "print(\"Quantized model is %d bytes\" % tflite_model_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JMt_dvsd1gU8"
      },
      "source": [
        "Notice how the quantized model is less than a third of the size of the float model? We'll next see how this reduction in size affects model accuracy. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwGBZvMV6hci"
      },
      "source": [
        "### TFLite Model Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fwV9zVq9liKo"
      },
      "outputs": [],
      "source": [
        "# Helper function to run inference\n",
        "def run_tflite_inference_test_set(tflite_model_path, model_type=\"Float\"):\n",
        "    # Load test data\n",
        "    test_data, test_labels = audio_processor.get_data(\n",
        "          -1, 0, BACKGROUND_FREQUENCY, BACKGROUND_VOLUME_RANGE,\n",
        "          TIME_SHIFT_MS, 'testing')\n",
        "    test_data = np.expand_dims(test_data, axis=1).astype(np.float32)\n",
        "\n",
        "    # Initialize the interpreter\n",
        "    interpreter = tf.lite.Interpreter(tflite_model_path)\n",
        "    interpreter.allocate_tensors()\n",
        "    input_details = interpreter.get_input_details()[0]\n",
        "    output_details = interpreter.get_output_details()[0]\n",
        "\n",
        "    # For quantized models, manually quantize the input data from float to integer\n",
        "    if model_type == \"Quantized\":\n",
        "        input_scale, input_zero_point = input_details[\"quantization\"]\n",
        "        test_data = test_data / input_scale + input_zero_point\n",
        "        test_data = test_data.astype(input_details[\"dtype\"])\n",
        "\n",
        "    # Evaluate the predictions\n",
        "    correct_predictions = 0\n",
        "    for i in range(len(test_data)):\n",
        "        interpreter.set_tensor(input_details[\"index\"], test_data[i])\n",
        "        interpreter.invoke()\n",
        "        output = interpreter.get_tensor(output_details[\"index\"])[0]\n",
        "        top_prediction = output.argmax()\n",
        "        correct_predictions += (top_prediction == test_labels[i])\n",
        "\n",
        "    print('%s model accuracy is %f%% (Number of test samples=%d)' % (\n",
        "        model_type, (correct_predictions * 100) / len(test_data), len(test_data)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JvSTTYczliKo"
      },
      "outputs": [],
      "source": [
        "# Compute float model accuracy\n",
        "run_tflite_inference_test_set(FLOAT_MODEL_TFLITE)\n",
        "\n",
        "# Compute quantized model accuracy\n",
        "run_tflite_inference_test_set(MODEL_TFLITE, model_type='Quantized')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HIC6SKf92DxT"
      },
      "source": [
        "As you can see, quantizing the model hardly affects the model accuracy in this case and makes it much more easily deployable to a tiny machine. \n",
        "\n",
        "We will now create the byte version of our model needed for deployment. \n",
        "\n",
        "**Replace the name of file in MODEL_TFLITE with the *quant* model you saved earlier in this notebook.**\n",
        "\n",
        "You can find the name of your file in \"{PROJECT_ROOT}/models/tflite_models\" under the folder icon to the left. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yerzdz4579rp"
      },
      "source": [
        "### TFLite Micro Conversion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ubkl7RJZliKo"
      },
      "outputs": [],
      "source": [
        "!apt-get update && apt-get -qq install xxd\n",
        "\n",
        "# Replace me!\n",
        "MODEL_TFLITE = TFLITE_DIR + '/quant_temp_0.tflite' \n",
        "MODEL_TFLITE_MICRO = MICRO_DIR+'/micro_models.cc'\n",
        "\n",
        "!xxd -i {MODEL_TFLITE} > {MODEL_TFLITE_MICRO}\n",
        "REPLACE_TEXT = MODEL_TFLITE.replace('/', '_').replace('.', '_')\n",
        "\n",
        "!sed -i 's/'{REPLACE_TEXT}'/g_model/g' {MODEL_TFLITE_MICRO}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UR-r0F0QauWN"
      },
      "source": [
        "You can find the below bites in the \"{PROJECT_ROOT}/models/micro_models\" folder. If you do not see an output after the next cell, make sure you replaced the file name  of {MODEL_TFLITE} in the cell above with your quant model created earlier in this notebook. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UpAEYG9cznZz",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "!cat {MODEL_TFLITE_MICRO}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lyHMrZJx1CHs"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "include_colab_link": true,
      "name": "4_model_conversion.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
