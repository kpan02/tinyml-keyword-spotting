{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ML-HW-SYS/a2/blob/main/6_pruning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rT9UCMkJpQt4"
      },
      "source": [
        "# **6. Model Pruning**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z69Judy_pm0f"
      },
      "source": [
        "## 6.0 Setup GDrive and Git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "lDjNQ68KByBz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make sure your token is stored in a txt file at the location below.\n",
        "# This way there is no risk that you will push it to your repo\n",
        "# Never share your token with anyone, it is basically your github password!\n",
        "with open('/content/gdrive/MyDrive/ece5545/token.txt') as f:\n",
        "    token = f.readline().strip()\n",
        "# Use another file to store your github username    \n",
        "with open('/content/gdrive/MyDrive/ece5545/git_username.txt') as f:\n",
        "    handle = f.readline().strip()"
      ],
      "metadata": {
        "id": "Xc4_W3YKB7g_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clone your github repo\n",
        "YOUR_TOKEN = token\n",
        "YOUR_HANDLE = handle\n",
        "BRANCH = \"main\"\n",
        "\n",
        "%mkdir /content/gdrive/MyDrive/ece5545\n",
        "%cd /content/gdrive/MyDrive/ece5545\n",
        "!git clone https://{YOUR_TOKEN}@github.com/ML-HW-SYS/a2-{YOUR_TOKEN}.git\n",
        "%cd /content/gdrive/MyDrive/ece5545/a2-{YOUR_TOKEN}\n",
        "!git checkout {BRANCH}\n",
        "!git pull\n",
        "%cd /content/gdrive/MyDrive/ece5545\n",
        "\n",
        "PROJECT_ROOT = f\"/content/gdrive/MyDrive/ece5545/a2-{YOUR_TOKEN}\""
      ],
      "metadata": {
        "id": "hCPU1IvvB7r6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This extension reloads all imports before running each cell\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "metadata": {
        "id": "aKMJj5f1B7zb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWpupyC8p7fD"
      },
      "source": [
        "### GPU: Ensure you are running the GPU runtime type:\n",
        "1.   Click \"Runtime\" on top banner\n",
        "2.   Select \"Change runtime type\"\n",
        "3.   Under \"Hardware accelarator\" select \"GPU\" and save"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Se1IC3sCqTYF"
      },
      "source": [
        "### Install required packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0VWKHppopJTu"
      },
      "outputs": [],
      "source": [
        "!pip install torchaudio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GbUE5nAHqhEo"
      },
      "source": [
        "### Import code dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bqEyUIUnpJTx"
      },
      "outputs": [],
      "source": [
        "# Import libraries \n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.nn.utils.prune as prune\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.notebook import tqdm\n",
        "import numpy as np\n",
        "import copy\n",
        "\n",
        "import time\n",
        "\n",
        "import sys\n",
        "\n",
        "# Adding assignment1 to the system path-- make sure this matches your git directory\n",
        "sys.path.insert(0, PROJECT_ROOT)\n",
        "\n",
        "# Import data_proc to use data processing functions\n",
        "import src.data_proc as data_proc\n",
        "\n",
        "# Import constants to use constants defined for training\n",
        "from src.constants import *\n",
        "\n",
        "# TensorBoard\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "# Set random seed\n",
        "# Make sure the shuffling and picking is deterministic\n",
        "# Note that different value of random_seed may change rate of variation in loss/accuracy during training\n",
        "# Using the same random seed value every time you rerun the notebook will \n",
        "# reproduce the training and testing results  \n",
        "random_seed = 0\n",
        "torch.manual_seed(random_seed)\n",
        "torch.cuda.manual_seed(random_seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tfyFWNsHpJTy"
      },
      "source": [
        "## 6.1 Prepare for Training\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NEBTHpmypJTz"
      },
      "outputs": [],
      "source": [
        "# Create audio_processor\n",
        "# DATASET_DIR is defined in constants.py\n",
        "audio_processor = data_proc.AudioProcessor()\n",
        "print(\"Audio_processor created\")\n",
        "\n",
        "# Define device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f'Using {device} to run the training scrpit.')\n",
        "\n",
        "# Define data loaders\n",
        "from src.loaders import make_data_loaders\n",
        "data_loaders = make_data_loaders(audio_processor, device)\n",
        "train_loader = data_loaders['training']\n",
        "test_loader = data_loaders['testing']\n",
        "valid_loader = data_loaders['validation']\n",
        "\n",
        "# Create a full precision (float32) TinyConv model\n",
        "from src.networks import TinyConv\n",
        "model_fp32 = TinyConv(model_settings=audio_processor.model_settings, \\\n",
        "    n_input=1, n_output=audio_processor.num_labels)\n",
        "\n",
        "model_fp32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "87-ko6koBE54"
      },
      "outputs": [],
      "source": [
        "!ls {TORCH_DIR}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p0kYnxYUBE54"
      },
      "source": [
        "### **TODO: Replace the torch_path model with the model you created in the last section.** \n",
        "\n",
        "You can find the name of your file in `TORCH_DIR` under the folder icon to the left. (Or from running the tab above)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZoRiIMnlBE55"
      },
      "outputs": [],
      "source": [
        "# TODO: Replace me!\n",
        "torch_path = os.path.join(TORCH_DIR, \"tinyconv_float32_init_seed0_90.28%_0.pt\")\n",
        "\n",
        "# Load model\n",
        "model_fp32.load_state_dict(torch.load(torch_path))\n",
        "model_fp32_orig = copy.deepcopy(model_fp32)\n",
        "model_fp32, model_fp32_orig"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ybjOCEoRpJT5"
      },
      "source": [
        "## 6.2 Structured Pruning\n",
        "\n",
        "In this section, you will try to conduct structured pruning on the TinyConv model and explore its effect on performance.\n",
        "In this notebook, you will be only given minimum scarfolding code. Please take advantages of the code in previous section to faciliate.\n",
        "\n",
        "Following link will be helpful:\n",
        "[torch.nn.utils.prune.LnStructured](https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.LnStructured.html?highlight=prune#torch.nn.utils.prune.LnStructured.prune)\n",
        "[Torch pruning tutorial](https://pytorch.org/tutorials/intermediate/pruning_tutorial.html?highlight=prune)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "EAbsnQD6LSYR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tv18oNx5BE56"
      },
      "source": [
        "## 6.3 Unstructured Pruning\n",
        "\n",
        "In this section, you will perform unstructured pruning on the TinyConv model and explore its effect on performance.\n",
        "\n",
        "Following link will be helpful:\n",
        "1. [torch.nn.utils.prune.l1_unstructured](https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.l1_unstructured.html?highlight=unstructured#torch.nn.utils.prune.l1_unstructured)\n",
        "\n",
        "2. [torch.nn.utils.prune.random_unstructured](https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.random_unstructured.html?highlight=unstructured#torch.nn.utils.prune.random_unstructured)\n",
        "\n",
        "2. [Torch pruning tutorial](https://pytorch.org/tutorials/intermediate/pruning_tutorial.html?highlight=prune)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W62TBfecBE56"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "6_pruning.ipynb",
      "provenance": [],
      "toc_visible": true,
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "interpreter": {
      "hash": "eed6bdaff5cc743acd241b8f3fe4c5dc3266475018a66ac615ca19ad2f28387d"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
